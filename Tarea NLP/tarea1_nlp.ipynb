{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Classificaton Fine-tuning \n",
    "\n",
    "La siguiente tarea consiste en entrenar un modelo de HuggingFace (HF) para realizar la _task_ de _classification_. El dataset para entrenar dicho modelo está predefinido. Sin embargo, el modelo, el tokenizador y el trainer pueden ser totalmente personalizados. Es decir, que tendréis que realizar un trabajo de investigación, de prueba y error, para poder ir aprendiendo y ganando destreza con HF. \n",
    "\n",
    "Recomendaciones: \n",
    "- Durante este proceso, tendréis muchas dudas y encontraréis muchos errores. Tratad de resolverlas primero por vuestra cuenta, enteniendo la causa del error. Después con recursos online. Y, finalmente, siempre está el foro, que puede ser utilizado de forma participativa.\n",
    "- No dejeis la tarea para el último día. Los modelos tardan en entrenar. Los problemas no se resuelven en la primera iteración.\n",
    "\n",
    "Finalmente, se pide:\n",
    "- Limpieza rigurosa en la presentación del notebook.\n",
    "- El notebook se entrega con todas las celdas ejecutadas.\n",
    "- Los comentarios (opcionales), mejor sobre el código con '#'. \n",
    "\n",
    "Ánimo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "A continuación, descargarás un dataset llamado _glue_ que contiene 392702 filas en el dataset de train y 9815 registros en el dataset de validation_match. \n",
    "\n",
    "Lo primero que tendrás que hacer es construir un dataset nuevo, llamado **ds_tarea**, que filtre el anterior dataset para quedarse con los registros que tengan el contenido de la columna _context_  con menos (estrictamente) de 20 caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270,
     "referenced_widgets": [
      "69caab03d6264fef9fc5649bffff5e20",
      "3f74532faa86412293d90d3952f38c4a",
      "50615aa59c7247c4804ca5cbc7945bd7",
      "fe962391292a413ca55dc932c4279fa7",
      "299f4b4c07654e53a25f8192bd1d7bbd",
      "ad04ed1038154081bbb0c1444784dcc2",
      "7c667ad22b5740d5a6319f1b1e3a8097",
      "46c2b043c0f84806978784a45a4e203b",
      "80e2943be35f46eeb24c8ab13faa6578",
      "de5956b5008d4fdba807bae57509c393",
      "931db1f7a42f4b46b7ff8c2e1262b994",
      "6c1db72efff5476e842c1386fadbbdba",
      "ccd2f37647c547abb4c719b75a26f2de",
      "d30a66df5c0145e79693e09789d96b81",
      "5fa26fc336274073abbd1d550542ee33",
      "2b34de08115d49d285def9269a53f484",
      "d426be871b424affb455aeb7db5e822e",
      "160bf88485f44f5cb6eaeecba5e0901f",
      "745c0d47d672477b9bb0dae77b926364",
      "d22ab78269cd4ccfbcf70c707057c31b",
      "d298eb19eeff453cba51c2804629d3f4",
      "a7204ade36314c86907c562e0a2158b8",
      "e35d42b2d352498ca3fc8530393786b2",
      "75103f83538d44abada79b51a1cec09e",
      "f6253931d90543e9b5fd0bb2d615f73a",
      "051aa783ff9e47e28d1f9584043815f5",
      "0984b2a14115454bbb009df71c1cf36f",
      "8ab9dfce29854049912178941ef1b289",
      "c9de740e007141958545e269372780a4",
      "cbea68b25d6d4ba09b2ce0f27b1726d5",
      "5781fc45cf8d486cb06ed68853b2c644",
      "d2a92143a08a4951b55bab9bc0a6d0d3",
      "a14c3e40e5254d61ba146f6ec88eae25",
      "c4ffe6f624ce4e978a0d9b864544941a",
      "1aca01c1d8c940dfadd3e7144bb35718",
      "9fbbaae50e6743f2aa19342152398186",
      "fea27ca6c9504fc896181bc1ff5730e5",
      "940d00556cb849b3a689d56e274041c2",
      "5cdf9ed939fb42d4bf77301c80b8afca",
      "94b39ccfef0b4b08bf2fb61bb0a657c1",
      "9a55087c85b74ea08b3e952ac1d73cbe",
      "2361ab124daf47cc885ff61f2899b2af",
      "1a65887eb37747ddb75dc4a40f7285f2",
      "3c946e2260704e6c98593136bd32d921",
      "50d325cdb9844f62a9ecc98e768cb5af",
      "aa781f0cfe454e9da5b53b93e9baabd8",
      "6bb68d3887ef43809eb23feb467f9723",
      "7e29a8b952cf4f4ea42833c8bf55342f",
      "dd5997d01d8947e4b1c211433969b89b",
      "2ace4dc78e2f4f1492a181bcd63304e7",
      "bbee008c2791443d8610371d1f16b62b",
      "31b1c8a2e3334b72b45b083688c1a20c",
      "7fb7c36adc624f7dbbcb4a831c1e4f63",
      "0b7c8f1939074794b3d9221244b1344d",
      "a71908883b064e1fbdddb547a8c41743",
      "2f5223f26c8541fc87e91d2205c39995"
     ]
    },
    "id": "s_AY1ATSIrIq",
    "outputId": "fd0578d1-8895-443d-b56f-5908de9f1b6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pablo\\anaconda3\\envs\\deep-learning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"mnli\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas en Train: ['premise', 'hypothesis', 'label', 'idx']\n",
      "Columnas en Validation Matched: ['premise', 'hypothesis', 'label', 'idx']\n"
     ]
    }
   ],
   "source": [
    "# Imprimir los nombres de las columnas para verificar cuál usar\n",
    "print(\"Columnas en Train:\", dataset['train'].column_names)\n",
    "print(\"Columnas en Validation Matched:\", dataset['validation_matched'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar el dataset para quedarse con los registros donde la longitud de 'context' es menor de 20 caracteres\n",
    "ds_tarea_train = dataset['train'].filter(lambda x: len(x['premise']) < 20)\n",
    "ds_tarea_validation = dataset['validation_matched'].filter(lambda x: len(x['premise']) < 20)\n",
    "\n",
    "# Crear un nuevo dataset combinando los subsets filtrados\n",
    "ds_tarea = {\n",
    "    'train': ds_tarea_train,\n",
    "    'validation_match': ds_tarea_validation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(ds_tarea['train']) == 13635\n",
    "assert len(ds_tarea['validation_match']) == 413"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Si tenéis que realizar alguna exploración del datos, utilizad esta sección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celdas de libre uso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Si tenéis que realizar alguna modificación de los datos (no siempre es necesaria, pero algunos modelos preentrenados lo piden), podéis utilizar esta sección. \n",
    "\n",
    "Al finalizar la sección, bien si modificais el dataset, bien si no lo modificáis, lo guardaréis en un dataset llamado __ds_tarea_featured__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celdas de libre uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tarea_featured = ds_tarea # Esta línea tiene sentido en caso de que no se modifique el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(ds_tarea_featured['train']) == 13635\n",
    "assert len(ds_tarea_featured['validation_match']) == 413"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Tokenizer\n",
    "\n",
    "El modelo finalmente escogido para hacer el fine-tuning, declaradlo en la variable _model_checkpoint_. Con dicho modelo seleccionado, se pide guardar el modelo y el tokenizador en las variables _model_ y _tokenizer_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5236, -0.0180,  0.8224]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de cómo tokenizar un texto de ejemplo\n",
    "text = \"Example text for tokenization and model testing.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n",
    "# Mostrar las logits de la salida del modelo\n",
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 413/413 [00:00<00:00, 1720.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    # Concatenate premise and hypothesis which are typical for NLI tasks\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Apply tokenization across the datasets\n",
    "ds_tarea_featured = {\n",
    "    \"train\": ds_tarea['train'].map(tokenize_function, batched=True),\n",
    "    \"validation_match\": ds_tarea['validation_match'].map(tokenize_function, batched=True)\n",
    "}\n",
    "\n",
    "# Optional: remove columns you don't need for training to optimize memory usage\n",
    "ds_tarea_featured[\"train\"] = ds_tarea_featured[\"train\"].remove_columns([\"premise\", \"hypothesis\", \"idx\"])\n",
    "ds_tarea_featured[\"validation_match\"] = ds_tarea_featured[\"validation_match\"].remove_columns([\"premise\", \"hypothesis\", \"idx\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "A continuación, de forma libre se pide entrenar un modelo de HuggingFace deseado. Se pide usar un Trainer de HuggingFace que tenga los siguientes argumentos como mínimo (puede haber más argumentos en todas las variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='models'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_tarea_featured[\"train\"],\n",
    "    eval_dataset=ds_tarea_featured[\"validation_match\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "A continuación se entrena el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 500/5115 [36:25<5:24:25,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8422, 'grad_norm': 28.83264923095703, 'learning_rate': 4.511241446725318e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1000/5115 [1:11:49<4:49:29,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6927, 'grad_norm': 7.9618072509765625, 'learning_rate': 4.0224828934506356e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 1500/5115 [1:47:14<4:15:39,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6408, 'grad_norm': 12.779501914978027, 'learning_rate': 3.533724340175953e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 2000/5115 [2:22:40<3:40:32,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5046, 'grad_norm': 7.472291946411133, 'learning_rate': 3.044965786901271e-05, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 2500/5115 [2:58:07<3:05:13,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4186, 'grad_norm': 12.568382263183594, 'learning_rate': 2.5562072336265887e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 3000/5115 [3:33:30<2:28:33,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4205, 'grad_norm': 43.60902786254883, 'learning_rate': 2.0674486803519064e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 3500/5115 [4:09:20<1:54:44,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3582, 'grad_norm': 8.665350914001465, 'learning_rate': 1.5786901270772238e-05, 'epoch': 2.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 4000/5115 [4:44:47<1:18:54,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2508, 'grad_norm': 0.17194288969039917, 'learning_rate': 1.0899315738025416e-05, 'epoch': 2.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 4500/5115 [5:20:14<43:40,  4.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2406, 'grad_norm': 110.2405014038086, 'learning_rate': 6.011730205278593e-06, 'epoch': 2.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 5000/5115 [5:55:42<08:04,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2346, 'grad_norm': 0.5464977025985718, 'learning_rate': 1.1241446725317694e-06, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5115/5115 [6:03:51<00:00,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 21831.2783, 'train_samples_per_second': 1.874, 'train_steps_per_second': 0.234, 'train_loss': 0.45510333966416466, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5115, training_loss=0.45510333966416466, metrics={'train_runtime': 21831.2783, 'train_samples_per_second': 1.874, 'train_steps_per_second': 0.234, 'total_flos': 2690663588040960.0, 'train_loss': 0.45510333966416466, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"modelo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "En este apartado, no vamos a entrar esta vez métrics. Lo que se va a pedir, es tomar dos ejemplos del dataset de evaluación. \n",
    "\n",
    "Con ambos ejemplos, vamos a ver cómo responden a las preguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = 55\n",
    "sample2 = 159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'Snap Judgment',\n",
       " 'hypothesis': 'Some judgments about race are made very quickly.',\n",
       " 'label': 1,\n",
       " 'idx': 1258}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise1 = ds_tarea['validation_match'][sample1]['premise']\n",
    "hypothesis1 = ds_tarea['validation_match'][sample1]['hypothesis']\n",
    "label1 = ds_tarea['validation_match'][sample1]['label']\n",
    "\n",
    "ds_tarea['validation_match'][sample1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'Trying Your Luck',\n",
       " 'hypothesis': 'Give it a try.',\n",
       " 'label': 0,\n",
       " 'idx': 3976}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise2 = ds_tarea['validation_match'][sample2]['premise']\n",
    "hypothesis2 = ds_tarea['validation_match'][sample2]['hypothesis']\n",
    "label2 = ds_tarea['validation_match'][sample2]['label']\n",
    "\n",
    "ds_tarea['validation_match'][sample2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se pide hacer la inferencia del modelo entrenado y poner los resultados en las variables _response1_ y _response2_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# Load the model and tokenizer\n",
    "model_path = \"modelo\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: {'premise': 'Snap Judgment', 'hypothesis': 'Some judgments about race are made very quickly.', 'true_label': 1, 'predicted_label': 1}\n",
      "Response 2: {'premise': 'Trying Your Luck', 'hypothesis': 'Give it a try.', 'true_label': 0, 'predicted_label': 0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Encode the inputs\n",
    "inputs1 = tokenizer(premise1, hypothesis1, return_tensors='pt', padding=True, truncation=True)\n",
    "inputs2 = tokenizer(premise2, hypothesis2, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1)\n",
    "    outputs2 = model(**inputs2)\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_label1 = torch.argmax(outputs1.logits, dim=1).item()\n",
    "predicted_label2 = torch.argmax(outputs2.logits, dim=1).item()\n",
    "\n",
    "response1 = {\n",
    "    \"premise\": premise1,\n",
    "    \"hypothesis\": hypothesis1,\n",
    "    \"true_label\": label1,\n",
    "    \"predicted_label\": predicted_label1\n",
    "}\n",
    "\n",
    "response2 = {\n",
    "    \"premise\": premise2,\n",
    "    \"hypothesis\": hypothesis2,\n",
    "    \"true_label\": label2,\n",
    "    \"predicted_label\": predicted_label2\n",
    "}\n",
    "\n",
    "print(\"Response 1:\", response1)\n",
    "print(\"Response 2:\", response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Question Answering on SQUAD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
